services:
  vllm:
    build:
      context: ./vllm
      dockerfile: Dockerfile
    container_name: llava-vllm-1
    gpus: all
    environment:
      - HF_HOME=/root/.cache/huggingface
      - HF_HUB_ENABLE_HF_TRANSFER=1
      - HF_HUB_DISABLE_TELEMETRY=1
      - HF_HUB_DISABLE_PROGRESS_BARS=1
      - PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
    volumes:
      - ./hf_cache:/root/.cache/huggingface
      - ./data:/data:ro
      - ./vllm/templates:/opt/templates:ro
    ports:
      - "9000:8000"
    entrypoint: ["/app/entrypoint.sh"]
    command:
      - "--model=llava-hf/llava-1.5-7b-hf"
      - "--chat-template=/opt/templates/template_llava.jinja"
      - "--trust-remote-code"
      - "--enforce-eager"
      - "--gpu-memory-utilization=0.75"
      - "--max-model-len=2048"
      - "--max-num-seqs=1"
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:8000/v1/models >/dev/null"]
      interval: 20s
      timeout: 10s
      start_period: 480s
      retries: 30
